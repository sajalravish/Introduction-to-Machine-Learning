{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would highly recommend looking at `neural_networks.grad_check.check_gradients` and making sure you understand how numerical gradient checking is being carried out. This function is used in the notebook to check the gradients of the neural network layers you write. Make sure to check the gradient of a layer after finishing its implementation.\n",
    "\n",
    "The function returns the relative error of the numerical gradient (approximated using finite differences) with respect to the analytical gradient (computed via backpropagation). Correct implementations should get very small errors, usually less than `1e-8` for 64-bit float matrices (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from neural_networks.utils import check_gradients\n",
    "from neural_networks.layers import FullyConnected, Conv2D\n",
    "from neural_networks.activations import Linear, Sigmoid, TanH, ReLU, SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for linear activation: 3.5997086139496045e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "linear_activation = Linear()\n",
    "_ = linear_activation.forward(X)\n",
    "grad = linear_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for linear activation:\",\n",
    "    check_gradients(\n",
    "        fn=linear_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m grad \u001b[38;5;241m=\u001b[39m sigmoid_activation\u001b[38;5;241m.\u001b[39mbackward(X, dLdY)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# check the gradients w.r.t. each parameter\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelative error for sigmoid activation:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmoid_activation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the function we are checking\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the analytically computed gradient\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# the variable w.r.t. which we are taking the gradient\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdLdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdLdY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# gradient at previous layer\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/coding/cs189/hw6/hw6_release/code/neural_networks/utils.py:90\u001b[0m, in \u001b[0;36mcheck_gradients\u001b[0;34m(fn, grad, x, dLdf, h)\u001b[0m\n\u001b[1;32m     88\u001b[0m oldval \u001b[38;5;241m=\u001b[39m x[ix]\n\u001b[1;32m     89\u001b[0m x[ix] \u001b[38;5;241m=\u001b[39m oldval \u001b[38;5;241m+\u001b[39m h\n\u001b[0;32m---> 90\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m     91\u001b[0m x[ix] \u001b[38;5;241m=\u001b[39m oldval \u001b[38;5;241m-\u001b[39m h\n\u001b[1;32m     92\u001b[0m neg \u001b[38;5;241m=\u001b[39m fn(x)\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "sigmoid_activation = Sigmoid()\n",
    "_ = sigmoid_activation.forward(X)\n",
    "grad = sigmoid_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for sigmoid activation:\",\n",
    "    check_gradients(\n",
    "        fn=sigmoid_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for tanh activation: 5.786241988396069e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "tanh_activation = TanH()\n",
    "_ = tanh_activation.forward(X)\n",
    "grad = tanh_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for tanh activation:\",\n",
    "    check_gradients(\n",
    "        fn=tanh_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for relu activation: 6.768641130783173e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "relu_activation = ReLU()\n",
    "out = relu_activation.forward(X)\n",
    "grad = relu_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for relu activation:\",\n",
    "    check_gradients(\n",
    "        fn=relu_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for softmax activation: 4.349684939231497e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "softmax_activation = SoftMax()\n",
    "_ = softmax_activation.forward(X)\n",
    "grad = softmax_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for softmax activation:\",\n",
    "    check_gradients(\n",
    "        fn=softmax_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Full Layers (Linear Activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for W: 1.321044206229347e-11\n",
      "Relative error for b: 3.01877174073028e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"linear\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "[[[[ 66.62591685  46.24740344 -64.58448283  23.97748331]\n",
      "   [-17.90429213  13.32476505 -41.33262016 -25.23690246]\n",
      "   [ 23.25190254  56.15870911  49.45500242  -4.17619592]\n",
      "   [ -1.40401741 -49.99028345 -35.02568745 -19.36803915]\n",
      "   [-36.04094659   3.8600905   26.98177987  -9.38675444]\n",
      "   [-41.4381196   10.25524817  14.95146523  -5.12946873]\n",
      "   [-49.59775806  10.09956853  31.37343488  -9.99651164]]\n",
      "\n",
      "  [[-15.68824579 -37.68145382  11.9697868   29.46383342]\n",
      "   [-60.04072086  -0.67707472 -16.42303504 -12.32614037]\n",
      "   [-67.50990262  34.79957572 -15.1830228   32.57102912]\n",
      "   [ 76.44677067 -39.2749448   23.37034711 -13.65308348]\n",
      "   [ 12.21475163  -2.63124483  52.89033729   0.54957487]\n",
      "   [ 28.45183759  40.27773483 -56.89804708   7.03766912]\n",
      "   [ 73.15987924  50.40485273 -51.46643878  14.41585306]]\n",
      "\n",
      "  [[ -8.41421398   8.25049691  34.85544481   7.97273434]\n",
      "   [  1.33375463  63.20364079 -62.07806337 -24.25114721]\n",
      "   [ -9.1496008   26.42549448  -9.92685681  10.29943576]\n",
      "   [ 26.11215219 -44.52983995  13.12955881  24.96070538]\n",
      "   [ 30.63523909 -48.19258363  -2.3292323  -33.89529087]\n",
      "   [-81.12667726 -19.16405058  18.82102888 -28.87669256]\n",
      "   [ 51.93133376  56.95940124  11.28488965  50.98731945]]]\n",
      "\n",
      "\n",
      " [[[-23.0196139  -34.97928259  29.74612639 -21.03649245]\n",
      "   [-47.31136779  30.32790111 -13.7831207   -5.95872737]\n",
      "   [-48.53362772   1.55854021   9.18972937 -50.76806572]\n",
      "   [-14.47147147  30.59133642   9.24394967 -32.73583447]\n",
      "   [ -6.43897694   2.13857499 -45.07658717  11.27736786]\n",
      "   [ -4.71257662  44.12983948 -71.39010292  38.45248114]\n",
      "   [ 10.31703948  -6.80045683  -5.18876754  13.54245434]]\n",
      "\n",
      "  [[ -1.99125893 -13.91518495  54.28252669 -17.80913623]\n",
      "   [-14.79135539  60.48250165 -26.23930405  -1.47966977]\n",
      "   [ 67.72156252  30.79472236 -34.98229744  53.04100546]\n",
      "   [ -6.29893889 -34.91185157  30.03564308  38.48561858]\n",
      "   [-55.60550145  13.3221138   39.1805643   20.53093598]\n",
      "   [ 52.96074958  41.33659702  41.27307724 -31.24015869]\n",
      "   [-53.9717606  -47.13276533 -29.16591754 -11.33984373]]\n",
      "\n",
      "  [[  6.03671733 -35.96686428  49.86108746 -40.57448092]\n",
      "   [ 23.96477797  51.54629077   2.73644262 -23.49786913]\n",
      "   [-47.2981424   -4.4595969   32.37371328  17.04181691]\n",
      "   [  4.70658749  12.87915524 -45.22339404  26.66242247]\n",
      "   [ 46.26399615  40.74588966  25.74411148 -13.22464613]\n",
      "   [ 21.08100118  12.40114699  -3.37564716  15.07489686]\n",
      "   [-14.01440754   9.23817718  28.44130718 -10.00666846]]]\n",
      "\n",
      "\n",
      " [[[  4.13987942 -49.24313532  27.0605364    4.09299494]\n",
      "   [ 38.69870925  39.12260302 -22.51010238  -1.7472982 ]\n",
      "   [ -4.03805112  45.25857421 -48.72910939  -6.60696407]\n",
      "   [-26.71431264  43.40460849 -28.02421332  13.63566617]\n",
      "   [  6.49218345  43.01937978  20.96069898  69.6905673 ]\n",
      "   [ 59.35399503  35.03791028 -25.67652827 -14.77217137]\n",
      "   [ 28.99704027  23.65748359  37.71814322  14.44414636]]\n",
      "\n",
      "  [[-14.27288189  26.73570993   2.26999117  34.62679848]\n",
      "   [ -4.85305638   4.67283428  35.63201765  27.11247223]\n",
      "   [-60.28594357 -31.76053508  -6.69665298 -11.33008917]\n",
      "   [ -3.53356946 -28.11425677  -8.78871521 -38.6189909 ]\n",
      "   [-18.39103852 -45.70272259  10.29496534  21.54481658]\n",
      "   [-38.67559712   7.39919861  57.65396651 -13.88883512]\n",
      "   [-30.93625406  52.14551121 -29.47891247 -24.58086958]]\n",
      "\n",
      "  [[ 57.08021381 -34.41293102 -31.30799281   7.75988644]\n",
      "   [-49.77618186 -32.04277644  36.62783296 -50.40845675]\n",
      "   [  6.23930446  42.64527153  16.72284558 -11.67479326]\n",
      "   [ 59.55767487   2.13688428  20.10562662 -29.80840865]\n",
      "   [ 26.55793938 -43.45729652  39.32306829 -45.88912866]\n",
      "   [-10.16116885  38.87433975  -2.5846981   45.26812257]\n",
      "   [-15.748163    13.59120411 -24.32702719  -4.54961833]]]]\n",
      "[[-0.231826    0.24257116 -0.01074517]\n",
      " [-0.06435851 -0.41287411  0.47723261]]\n",
      "Relative error for W: None None None 1.0135848903411233\n",
      "b\n",
      "[[ -2.33735694 -10.9219602    1.01913002   0.7160234 ]]\n",
      "[[-0.231826    0.24257116 -0.01074517]\n",
      " [-0.06435851 -0.41287411  0.47723261]]\n",
      "Relative error for b: None None None 5.799876430331493e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 5, 6, 7)\n",
    "dLdY = np.random.randn(2, 5, 6, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "conv_layer = Conv2D(\n",
    "    n_out=4,\n",
    "    kernel_shape=(3, 3),\n",
    "    activation=\"linear\",\n",
    "    weight_init=\"uniform\",\n",
    "    pad=\"same\",\n",
    ")\n",
    "_ = conv_layer.forward(X)\n",
    "_ = conv_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in conv_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        print(param),\n",
    "        print(conv_layer.gradients[param]),\n",
    "        print(grad),\n",
    "        check_gradients(\n",
    "            fn=conv_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=conv_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=conv_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.losses import CrossEntropy\n",
    "\n",
    "num_pts = 5\n",
    "num_classes = 6\n",
    "\n",
    "# one-hot encoded y\n",
    "y_idxs = np.random.randint(0, num_classes, (num_pts,))\n",
    "y = np.zeros((num_pts, num_classes))\n",
    "y[range(num_pts), y_idxs] = 1\n",
    "\n",
    "# normalized predictions\n",
    "scores = np.random.uniform(0, 1, size=(num_pts, num_classes))\n",
    "y_hat = scores / scores.sum(axis=1, keepdims=True)\n",
    "\n",
    "cross_entropy_loss = CrossEntropy(\"cross_entropy\")\n",
    "\n",
    "def forward_fn(Y, Y_hat):    \n",
    "    def inner_forward(Y_hat):\n",
    "        return cross_entropy_loss.forward(Y, Y_hat)\n",
    "    return inner_forward\n",
    "\n",
    "loss = cross_entropy_loss.forward(y, y_hat)\n",
    "grad = cross_entropy_loss.backward(y, y_hat)\n",
    "\n",
    "print(\n",
    "    f\"Relative error for cross entropy loss:\",\n",
    "    check_gradients(\n",
    "        fn=forward_fn(y, y_hat),  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=y_hat,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=1,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
